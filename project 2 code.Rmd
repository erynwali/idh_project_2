---
title: "Project 2 code"
author: "Eryn Wali"
date: "07/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r error=FALSE, eval= FALSE, warning=FALSE, message=FALSE}
install.packages("rJava")
install.packages("openNLP")
install.packages("NLP")
```

```{r echo=TRUE, eval= FALSE, error=FALSE, warning=FALSE, message=FALSE}
install.packages("openNLPmodels.en", dependencies=TRUE, repos = "http://datacube.wu.ac.at/")
```

```{r echo=TRUE, results='hide', message=FALSE}
#NLP Libraries
library(rJava)
library(openNLP)
library(NLP)


#Tidy data manipulation
library(stringr)
library(dplyr)
library(tidyr)
library(tidytext)
library(readr)
library(stringi)
library(textclean)

#Corpus ingest
library(gutenbergr)

#Helper library
library(sqldf)

#Graphics library
library(ggiraphExtra)
library(ggplot2)
library(RColorBrewer)
library(scales)
```

```{r}
corpus <-
  gutenberg_download(c(2554, 135, 98, 2701, 4276, 29220, 19734, 1342, 1260, 145, 21993, 8677),
                     mirror = "http://mirrors.xmission.com/gutenberg/",
                     meta_fields = c("author", "title"))
```



```{r pre_clean_corpus}
corpus_clean <- corpus %>%
  filter(text != "") %>%
  mutate(text = str_replace_all(text, "_", " ")) %>%
  mutate(text = replace_contraction(text)) %>%
  mutate(text = replace_curly_quote(text))
```

```{r create_strings}
corpus_text <- corpus_clean %>%
  group_by(title) %>%
  mutate(text = paste(as.character(text), collapse = " ")) %>%
  distinct() %>%
  ungroup()
```


```{r create_nested_string}
corpus_text_str <- corpus_text %>%
  group_by(title) %>%
  mutate(text = list(as.String(text)))
```

```{r initiate_pipeline}
#set pipeline
wordAnnotator <- Maxent_Word_Token_Annotator(language = "en")
sentenceAnnotator <- Maxent_Sent_Token_Annotator(language = "en")
characterAnnotatorEN <- Maxent_Entity_Annotator(language = "en", kind = "person")
locationAnnotatorEN <- Maxent_Entity_Annotator(language = "en", kind = "location")

pipeline <- list(sentenceAnnotator,
                 wordAnnotator,
                 characterAnnotatorEN,
                 locationAnnotatorEN)
```

```{r NER_chunker, message=FALSE}
#create empty df
full_df = as.data.frame(NULL)
chunk_size = 10000

for (j in 1:nrow(corpus_text_str)) {
  #get number of chunks
  chunk <- nchar(corpus_text_str$text[j]) %/% chunk_size
  text <- unlist(corpus_text_str$text[j])
  text <- as.String(text)
  
  #Loop runs through the text section by section and reads each chunk into a df
  
  for (i in 1:chunk) {
    print(paste0(
      "Processing title: ",
      corpus_text_str$title[j],
      " - section ",
      i,
      " of ",
      chunk
    ))
    temp_df = NULL
    
    if (i == 1) {
      m = 1
    }
    
    if (i == chunk) {
      m = n + 1
      n = (nchar(text))
    }
    else{
      n <- m + chunk_size
    }
    
    temp_string = text[m, n]
    
    temp_ann <- NLP::annotate(temp_string, pipeline)
    
    temp_df <-  temp_ann %>%
      as.data.frame %>% 
      filter(type != "word")
    
    temp_df <- temp_df %>%
      mutate(words = str_sub(
        as.character(temp_string),
        start = temp_df$start,
        end = temp_df$end
      )) %>%
      unnest_wider(features)
    
    temp_df <- temp_df %>%
      mutate(author = corpus_text_str$author[j], title = corpus_text_str$title[j]) 
      #This is where you would include your added variable
      
    
    #stitch it all together
    full_df <- full_df %>%
      bind_rows(temp_df)
    
    m <- m + chunk_size
  }
}

```

```{r create_backup}
full_df_backup <- full_df
``` 

```{r rough_clean}
full_df <-  full_df %>%
  mutate(words = str_remove_all(words, '[:punct:]'))
```

```{r reshape_df}
full_df <- full_df %>%
  relocate(c("author", "title"), .before = 1) %>%
  select(-id,-constituents) 
```

```{r backup_annotations, results='hide'}
write_csv(full_df, "annotation_backup.csv") 
```

```{r}
#full_df <- read_csv("annotation_backup.csv")
``` 

```{r split_sentence_entity}
df1 <- full_df %>%
  filter(type == "sentence") %>%
  mutate(sentence_nr = row_number()) %>%
  select(author, title, words, sentence_nr) %>%
  rename(sentence = words)

df2 <-  full_df %>%
  filter(type == "entity") %>%
  mutate(record = row_number()) %>%
  select(words, kind)
```

```{r computational_cleaning}
df2 <- df2 %>%
  mutate(words = str_replace_all(words, "Dear ", "")) %>%
  mutate(words = str_replace_all(words, "Young ", "")) %>%
  mutate(words = str_replace_all(words, "Ah", "")) %>%
  mutate(words = str_replace_all(words, "Oh", "")) %>%
  mutate(words = str_trim(words, side = "both")) %>%
  mutate(words = str_trim(gsub("[A-Z]{2,}", "", words))) %>%
  mutate(words = str_squish(words)) %>%
  mutate_all( ~ ifelse(. %in% c("N/A", "null", ""), NA, .)) %>%
  drop_na() %>%
  dplyr::filter(nchar(words) > 2) %>%
  distinct()

capital_stopwords <-
  as.data.frame(str_to_title(stop_words$word)) %>%
  rename(words = 1)

df2 <- df2 %>%
  anti_join(capital_stopwords)
``` 

```{r backup_join}
write_csv(df2, "pre_join_entities.csv")
```

```{r load_cleaned_data}
#Prep the data
pre_join <- read_csv("pre_join_entities_clean.csv", na = "NA")
``` 

```{r limit_join_words}
pre_join <- pre_join %>%
  select(words, kind) %>%
  drop_na()  %>%
  distinct()
``` 

```{r match_dataframes}
#Execute a SQL query
full_join_df <- sqldf("SELECT *
      FROM df1
      LEFT JOIN pre_join ON df1.sentence LIKE '%' || pre_join.words || '%'")

full_join_df <- full_join_df %>%
  distinct()
``` 

```{r}
write_csv(full_join_df, "entities_raw.csv")
``` 

```{r}
clean_entities <- read.csv("entities_cleaned.csv")
``` 

```{r unnest}
entities_unnest <- clean_entities %>%
  unnest_tokens(word, sentence)
``` 

```{r remove_stopwords}
entities_unnest <- entities_unnest %>%
  anti_join(stop_words)
``` 

```{r message=FALSE}
#create sentiment table
entities_sentiment <- entities_unnest %>%
  group_by(author, title) %>%
  inner_join(get_sentiments("nrc")) %>%
  count(sentence_nr, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
``` 

```{r message=FALSE}
entities_matches_sentiment <- entities_unnest %>%
  inner_join(entities_sentiment) %>%
  distinct_at(vars(-word))  
  
``` 

```{r message=FALSE}
ner_total_sentiment <- entities_matches_sentiment %>%
  group_by(author, title, words, kind) %>%
  summarise(total = mean(sentiment), appearance = n())  
``` 



